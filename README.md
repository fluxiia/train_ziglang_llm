

## Documenta√ß√£o do Processo: Treinando um LLM para Zig 0.14

Com base nas √∫ltimas postagens do Akita em seu blog, decidi tentar ajustar um modelo **Qwen 2.5 3B** para entender a linguagem **Zig**, especificamente a vers√£o **0.14**.

Minha inten√ß√£o aqui √© documentar o m√°ximo poss√≠vel de cada passo que executei at√© chegar no resultado final ‚Äî come√ßando, inclusive, **pelo final**.

### Objetivo Final

Precisamos de uma forma objetiva para avaliar a qualidade das respostas geradas pelo modelo. Nosso **benchmark principal** ser√°:

> ‚úÖ *"O c√≥digo compila e executa corretamente a tarefa solicitada."*

Para isso, vamos utilizar o **[Codapi](https://www.codapi.org/)**, que compila e executa c√≥digo Zig (entre outras linguagens) diretamente via API. A vers√£o utilizada pelo Codapi j√° est√° atualizada para Zig 0.14, ent√£o ela servir√° tanto para validar os dados gerados quanto como base para construir o dataset.

### Estrat√©gia Geral

* **Valida√ß√£o Automatizada:** Utilizaremos o Codapi para testar os c√≥digos gerados pelo modelo e determinar se s√£o v√°lidos.
* **Divis√£o do Dataset:** O dataset ser√° dividido em treino e avalia√ß√£o. Apesar de termos poucos dados, queremos medir a capacidade de **generaliza√ß√£o** do modelo.
* **Escassez de Dados:** LLMs s√£o treinados com *terabytes* de dados. Mesmo os processos de alinhamento envolvem *milh√µes* de exemplos. Aqui faremos um experimento com um volume muito menor, focando em qualidade e valida√ß√£o pr√°tica.

### Abordagem T√©cnica

1. **Documenta√ß√£o como Fonte:** Vamos usar o arquivo `zig-0.14.0-docs.md` como ponto de partida.
2. **Gera√ß√£o de Dados Sint√©ticos:**

   * Dividir o arquivo em se√ß√µes menores.
   * Para cada se√ß√£o, gerar **pares de perguntas e respostas** usando um LLM (ex: "Como funciona defer?", "Escreva um exemplo com error union").
3. **Valida√ß√£o Autom√°tica:**

   * Executar cerca de **5% dos c√≥digos** gerados no Codapi para verificar se s√£o **funcionais**.
4. **Estrutura baseada em Agentes:**

   * Um agente para **ler e dividir** o conte√∫do.
   * Um agente para **gerar os pares**.
   * Um agente para **validar os c√≥digos** via Codapi.


### Etapa 1 ‚Äî Dividindo a Documenta√ß√£o em Se√ß√µes

Para facilitar o processo de gera√ß√£o de perguntas e respostas, precisamos dividir a documenta√ß√£o oficial do Zig 0.14 (`zig-0.14.0-docs.md`) em se√ß√µes menores e bem delimitadas.

O script abaixo realiza exatamente isso:

* Identifica t√≠tulos e subt√≠tulos (n√≠veis `#`, `##` e `###`).
* Agrupa o conte√∫do de cada se√ß√£o em objetos separados.
* Salva o resultado em dois arquivos:

  * `zig_sections.jsonl`: uma linha por se√ß√£o com o campo `"text"`.
  * `zig_sections_stats.json`: informa√ß√µes √∫teis como t√≠tulo, n√≠vel e tamanho da se√ß√£o.

#### C√≥digo:

```python
import re
import json
import os

def split_markdown_by_sections(markdown_content):
    """
    Divide o conte√∫do markdown em se√ß√µes baseadas nos cabe√ßalhos.
    Retorna uma lista de dicion√°rios com o conte√∫do de cada se√ß√£o.
    """
    # Regex para identificar cabe√ßalhos markdown (# T√≠tulo, ## Subt√≠tulo, etc)
    section_pattern = r'^(#{1,3})\s+(.+)$'
    sections = []
    current_section = {"title": "Intro", "level": 0, "content": ""}
    
    for line in markdown_content.split('\n'):
        match = re.match(section_pattern, line)
        if match:
            # Se o conte√∫do da se√ß√£o atual n√£o estiver vazio, salva a se√ß√£o
            if current_section["content"].strip():
                sections.append(current_section)
            
            # Inicia uma nova se√ß√£o
            level = len(match.group(1))
            title = match.group(2)
            current_section = {
                "title": title, 
                "level": level, 
                "content": f"{match.group(1)} {title}\n"
            }
        else:
            # Continua adicionando √† se√ß√£o atual
            current_section["content"] += line + "\n"
    
    # Adiciona a √∫ltima se√ß√£o se tiver conte√∫do
    if current_section["content"].strip():
        sections.append(current_section)
    
    return sections

def save_sections_to_jsonl(sections, output_file):
    """
    Salva as se√ß√µes em formato JSONL com a estrutura: {"text": conte√∫do}
    """
    with open(output_file, "w", encoding="utf-8") as f:
        for section in sections:
            # Cada linha √© um objeto JSON com o campo "text" contendo o conte√∫do da se√ß√£o
            json_line = json.dumps({"text": section["content"].strip()})
            f.write(json_line + "\n")
    
    print(f"Foram criadas {len(sections)} se√ß√µes e salvas em {output_file}")

def main():
    input_file = "zig-0.14.0-docs.md.txt"
    output_file = "zig_sections.jsonl"
    
    # Verifica se o arquivo de entrada existe
    if not os.path.exists(input_file):
        print(f"Erro: O arquivo {input_file} n√£o foi encontrado.")
        return
    
    # L√™ o conte√∫do do arquivo
    with open(input_file, "r", encoding="utf-8") as f:
        content = f.read()
    
    # Divide o documento em se√ß√µes
    sections = split_markdown_by_sections(content)
    
    # Salva as se√ß√µes em formato JSONL
    save_sections_to_jsonl(sections, output_file)
    
    # Salva tamb√©m estat√≠sticas sobre as se√ß√µes em um arquivo separado
    stats = [{"id": i, "title": s["title"], "level": s["level"], "size": len(s["content"])} 
             for i, s in enumerate(sections)]
    
    with open("zig_sections_stats.json", "w", encoding="utf-8") as f:
        json.dump(stats, f, indent=2)
    
    print(f"Estat√≠sticas das se√ß√µes salvas em zig_sections_stats.json")

if __name__ == "__main__":
    main()

```

#### Exemplo de sa√≠da (`zig_sections.jsonl`):

```json
{"text": "# Introduction\nZig is a general-purpose programming language..."}
{"text": "## Error Handling\nZig has a unique approach to error handling..."}
```

#### Exemplo de sa√≠da (`zig_sections_stats.json`):

```json
[
  {"id": 0, "title": "Introduction", "level": 1, "size": 1024},
  {"id": 1, "title": "Error Handling", "level": 2, "size": 893}
]
```

## Etapa 2 ‚Äî Gera√ß√£o de Dados Sint√©ticos com LLM (via Novita)

Agora que temos a documenta√ß√£o do Zig 0.14 dividida em se√ß√µes, o pr√≥ximo passo √© gerar exemplos de **perguntas e respostas (instruction-response pairs)** que sirvam como dados de fine-tuning ou alinhamento.

### Plataforma de IA

Utilizamos a [**Novita.ai**](https://novita.ai/referral?invited_code=NBDEJX), uma alternativa √† OpenAI que oferece compatibilidade com a API padr√£o. Com registro via GitHub e o link acima, voc√™ recebe at√© **\$15 em cr√©ditos** ‚Äî o suficiente para gerar milhares de exemplos para nosso dataset.

### Estrat√©gia de Gera√ß√£o

* Para cada se√ß√£o da documenta√ß√£o:

  * Geramos de 2 a 3 pares de QA (perguntas conceituais e pedidos de gera√ß√£o de c√≥digo).
  * Usamos o modelo `meta-llama/llama-3.1-8b-instruct` via Novita.
  * O formato gerado √© **JSONL**, com campos `"instruction"` e `"response"`.
* Se a se√ß√£o for muito longa, o conte√∫do √© processado em at√© **3 itera√ß√µes**, com continuidade baseada na √∫ltima resposta.

### Estrutura de Prompt

O prompt fornece **somente o conte√∫do da documenta√ß√£o**, sem permitir que o modelo use conhecimentos externos, garantindo que os pares sejam realmente baseados no material oficial do Zig 0.14.

### C√≥digo

```python
import json
from openai import OpenAI
import time

# Configura√ß√£o do cliente OpenAI com a API Novita
client = OpenAI(
    base_url="https://api.novita.ai/v3/openai",
    api_key="<API_NOVITA>",
)

def generate_qa_pairs(zig_content, long=False, last_response=None):
    """
    Gera pares de instru√ß√£o-resposta usando o LLM baseados no conte√∫do Zig fornecido.
    
    Args:
        zig_content (str): Conte√∫do da documenta√ß√£o Zig a ser processado
        long (bool): Se True, indica que estamos processando um conte√∫do longo em partes
        last_response (str): A √∫ltima resposta recebida, usada para continuidade em conte√∫dos longos
    
    Returns:
        tuple: (conte√∫do_gerado, is_long) onde is_long indica se ainda h√° mais conte√∫do a ser processado
    """
    model = "meta-llama/llama-3.1-8b-instruct"
    max_tokens = 8192
    
    # Preparar o prompt para o LLM
    system_message = "You are a Zig language expert who creates instruction-response pairs to train language models. Use only the information provided, without adding external knowledge."
    
    if long and last_response:
        # Se estamos continuando um processamento longo, usamos um prompt diferente
        prompt = f"""
Continue generating instruction-response pairs based on the same Zig documentation content.
You previously generated the following response:

{last_response}

Now, generate 2-3 more instruction-response pairs using the same format but covering different aspects or examples.

Respond ONLY in JSONL format, with each line containing an object with the fields "instruction" and "response":
{{"instruction": "Question or instruction", "response": "Detailed response or code"}}
{{"instruction": "Another question or instruction", "response": "Another detailed response or code"}}
"""
    else:
        # Prompt original para o primeiro processamento
        prompt = f"""
Based on this content from the Zig language documentation, generate 2-3 instruction-response pairs.
The pairs can be questions about language concepts OR instructions to generate code.

For conceptual questions, create complete and clear questions that can be answered using only the information provided.
For code generation, create specific instructions requesting implementations that demonstrate the concepts covered.
Use only the text below from the Zig 0.14 documentation to generate the questions and answers.

ZIG DOCUMENTATION:
{zig_content}

Respond ONLY in JSONL format, with each line containing an object with the fields "instruction" and "response":
{{"instruction": "Question or instruction", "response": "Detailed response or code"}}
{{"instruction": "Another question or instruction", "response": "Another detailed response or code"}}
"""

    try:
        # Enviar para o LLM
        completion = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": system_message
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            max_tokens=max_tokens,
            temperature=0.7,
            stream=False
        )
        
        content = completion.choices[0].message.content.strip()
        
        # Determinar se o conte√∫do ainda √© longo e precisa de mais processamento
        # Assumimos que ainda √© longo se o conte√∫do original tiver mais de 5000 caracteres
        # e j√° tivermos processado menos de 3 itera√ß√µes (ajuste esses valores conforme necess√°rio)
        is_still_long = long and len(zig_content) > 5000
        
        return content, is_still_long
    except Exception as e:
        print(f"Erro ao gerar QA pairs: {e}")
        return "", False

def main():
    input_file = "zig_sections_improved.jsonl"
    output_file = "zig_dataset.jsonl"
    
    processed = 0
    success = 0
    
    # Abrir o arquivo de sa√≠da para escrita
    with open(output_file, "w", encoding="utf-8") as outfile:
        # Ler o arquivo JSONL linha por linha
        with open(input_file, "r", encoding="utf-8") as infile:
            for line_num, line in enumerate(infile):
                section = json.loads(line)
                content = section["text"]
                
                # Verificar se o conte√∫do √© significativo (mais de 200 caracteres)
                if len(content) > 200:
                    print(f"Processando se√ß√£o {line_num + 1}...")
                    
                    # Vari√°veis para processamento de conte√∫do longo
                    is_long = len(content) > 5000  # Consideramos longo se tiver mais de 5000 caracteres
                    iteration = 0
                    last_response = None
                    all_qa_pairs = []
                    
                    # Loop para processar o conte√∫do em partes se for longo
                    while True:
                        # Gerar pares de QA, possivelmente continuando de uma resposta anterior
                        qa_pairs, still_long = generate_qa_pairs(content, long=is_long, last_response=last_response)
                        
                        print(f"Itera√ß√£o {iteration + 1}:")
                        print(qa_pairs)
                        
                        if qa_pairs:
                            # Remover marca√ß√µes de blocos de c√≥digo ```jsonl ou ``` se presentes
                            cleaned_qa_pairs = qa_pairs.replace("```jsonl", "").replace("```", "")
                            all_qa_pairs.append(cleaned_qa_pairs)
                            last_response = qa_pairs  # Salvar a vers√£o original para a pr√≥xima itera√ß√£o
                        
                        iteration += 1
                        
                        # Se n√£o for mais longo ou j√° tivermos feito 3 itera√ß√µes, paramos
                        if not still_long or iteration >= 3:
                            break
                    
                    # Escrever todos os pares no arquivo de sa√≠da
                    if all_qa_pairs:
                        # Garantir que n√£o haja marca√ß√µes de c√≥digo no arquivo final
                        final_output = "\n\n".join(all_qa_pairs).strip()
                        outfile.write(final_output + "\n\n")
                        success += 1
                    
          
                
                processed += 1
                
                # Imprimir progresso a cada 10 itens
                if processed % 10 == 0:
                    print(f"Progresso: {processed} se√ß√µes processadas, {success} com sucesso")
    
    print(f"\nProcessamento conclu√≠do! {processed} se√ß√µes processadas, {success} com sucesso")
    print(f"Dataset salvo em {output_file}")

if __name__ == "__main__":
    main()

```

### Exemplo de sa√≠da esperada:

```json
{"instruction": "Explique como funciona o comando `defer` em Zig.", "response": "O `defer` em Zig agenda uma instru√ß√£o para execu√ß√£o no final do escopo atual..."}
{"instruction": "Implemente uma fun√ß√£o Zig que usa `defer` para fechar um arquivo.", "response": "const std = @import(\"std\");\n..."}
```


---
Claro! Aqui est√° seu conte√∫do reestruturado, organizado em t√≥picos claros, com uma linguagem mais fluida e did√°tica ‚Äî ideal para um post t√©cnico ou tutorial:

---

## Entendendo Chat Template e o Padr√£o ChatML em Modelos Instru√ß√£o

Ao treinar ou ajustar modelos de linguagem (LLMs) para intera√ß√µes em formato de chat, um elemento essencial √© o **chat template** ‚Äî respons√°vel por transformar as mensagens do usu√°rio, sistema e assistente em um formato que o modelo consegue compreender e gerar a pr√≥xima resposta corretamente.

### O Que √© o ChatML?

O ChatML √© um dos padr√µes de formata√ß√£o de mensagens mais utilizados hoje, especialmente por modelos derivados da OpenAI e suas variantes open-source. Ele funciona com marca√ß√µes expl√≠citas para cada papel no di√°logo, como este exemplo:

```txt
<|im_start|>user
Can I ask a question?<|im_end|>
```

### Como Funciona na Pr√°tica

Quando utilizamos uma API que segue o padr√£o OpenAI (como a da Novita), normalmente enviamos as mensagens no formato:

```python
messages = [
    {"role": "system", "content": "Prompt de sistema"},
    {"role": "user", "content": "Solicita√ß√£o do usu√°rio"}
]
```

Internamente, o tokenizador converte isso em algo como:

```txt
<|im_start|>system
Prompt de sistema<|im_end|>
<|im_start|>user
Solicita√ß√£o do usu√°rio<|im_end|>
<|im_start|>assistant
```

O modelo ent√£o **gera token por token** a partir desse ponto, completando o papel do assistente. O processo para quando ele encontra o **token de parada**, chamado `eos_token`, que nesse caso √© `<|im_end|>`.

### Personalizando o Chat Template

Se quiser, voc√™ pode **criar pap√©is personalizados** no template, como `docs`, `tool`, `function`, entre outros. Isso √© muito √∫til para tarefas como:

* Passar trechos de documenta√ß√£o.
* Simular chamadas de ferramentas.
* Informar dados intermedi√°rios no racioc√≠nio do modelo.

### Onde Encontrar o Template?

Voc√™ pode descobrir o `chat_template` de qualquer modelo no Hugging Face acessando o arquivo `tokenizer_config.json` do reposit√≥rio. L√°, procure pela chave `chat_template`.

Esses templates s√£o escritos em **Jinja2**, uma linguagem de templates com l√≥gica condicional. Aqui est√° um trecho do template do modelo **Qwen 2.5**, da Alibaba:

```jinja2
{% for message in messages %}
  {% if message.role == "user" %}
    <|im_start|>user
    {{ message.content }}<|im_end|>
  {% elif message.role == "assistant" %}
    <|im_start|>assistant
    {{ message.content }}<|im_end|>
  {% endif %}
{% endfor %}
```

No caso do Qwen 2.5 com suporte a ferramentas, o template ainda trata `tool_calls`, `tool_responses`, e outros pap√©is especiais.

### Importante: Apenas Modelos Instru√ß√£o T√™m Chat Template

Os **modelos base (base models)**, ou seja, n√£o-instru√≠dos, **n√£o possuem chat template**. Eles s√£o treinados com tarefas de preenchimento de texto ‚Äî ou seja, se voc√™ der a eles:

```
Zig is a low-level language that...
```

Eles v√£o apenas tentar **completar** esse texto, e n√£o responder uma pergunta como um assistente. J√° os modelos instru√≠dos (como LLaMA-2-chat, Mistral-instruct, Qwen-chat) respondem comandos e seguem conversas estruturadas.

---

## Convertendo Nosso Dataset para o Padr√£o ChatML

Agora que entendemos o funcionamento do chat template, o pr√≥ximo passo do projeto √© transformar o nosso dataset de perguntas e respostas em um formato compat√≠vel com esse padr√£o, como o ChatML. Para isso:

* Cada item do dataset ser√° transformado em uma sequ√™ncia com:

  * Um `system prompt` (opcional, se houver).
  * Um `user message` com a instru√ß√£o.
  * Um `assistant message` com a resposta.

### Exemplo:

#### Original (instru√ß√£o e resposta):

```json
{
  "instruction": "Explique como funciona o comando `defer` em Zig.",
  "response": "O `defer` agenda uma opera√ß√£o para o fim do escopo atual..."
}
```

#### Convertido para ChatML:

```txt
<|im_start|>user
Explique como funciona o comando `defer` em Zig.<|im_end|>
<|im_start|>assistant
O `defer` agenda uma opera√ß√£o para o fim do escopo atual...<|im_end|>
```

Isso garante que o modelo treinado ou ajustado saiba exatamente qual mensagem √© do usu√°rio e qual √© a resposta esperada.
Perfeito! Essa √© a terceira etapa do seu pipeline ‚Äî **preparar o dataset para fine-tuning**, convertendo para o formato esperado pelo `unsloth`, que adota a estrutura `{"conversations": [...]}` em JSONL.

---

## Etapa 3 ‚Äî Convers√£o para o Formato ChatML (Compat√≠vel com Unsloth)

Depois de gerar os pares de **instru√ß√£o e resposta**, o pr√≥ximo passo √© preparar o dataset no **formato esperado por frameworks como o `unsloth`**, que utilizam o padr√£o ChatML para treinar modelos de linguagem com base em conversas.

### Objetivo

Transformar este tipo de entrada:

```json
{"instruction": "What is the purpose of doc comments in Zig?", "response": "Doc comments in Zig are used to provide documentation for the code..."}
```

Em algo assim:

```json
{"conversations": [
  {"role": "user", "content": "What is the purpose of doc comments in Zig?"},
  {"role": "assistant", "content": "Doc comments in Zig are used to provide documentation for the code..."}
]}
```

### Script de Convers√£o

Abaixo est√° o script completo que realiza a convers√£o:

```python
import json
import re

def convert_to_chatml(input_file, output_file):
    """
    Convert the zig_dataset.jsonl file to chatml format.
    Input: JSONL com {"instruction": "...", "response": "..."}
    Output: JSONL com {"conversations": [{"role": "user", ...}, {"role": "assistant", ...}]}
    """
    chatml_data = []
    
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    content = re.sub(r'(\{[^\{\}]*?)\n([^\{\}]*?\})', r'\1 \2', content, flags=re.DOTALL)
    
    json_objects = []
    current_obj = ""
    in_obj = False
    
    for line in content.splitlines():
        line = line.strip()
        if not line:
            continue
            
        if line.startswith('{') and not in_obj:
            current_obj = line
            in_obj = True
            if line.endswith('}'):
                json_objects.append(current_obj)
                current_obj = ""
                in_obj = False
        elif line.endswith('}') and in_obj:
            current_obj += " " + line
            json_objects.append(current_obj)
            current_obj = ""
            in_obj = False
        elif in_obj:
            current_obj += " " + line
    
    for obj in json_objects:
        try:
            data = json.loads(obj)
            chatml_conversation = {
                "conversations": [
                    {"role": "user", "content": data["instruction"]},
                    {"role": "assistant", "content": data["response"]}
                ]
            }
            chatml_data.append(chatml_conversation)
        except json.JSONDecodeError as e:
            print(f"Erro ao interpretar JSON: {e}")
            print(f"Objeto problem√°tico: {obj[:100]}...")
        except KeyError as e:
            print(f"Chave ausente: {e}")
            print(f"Objeto problem√°tico: {obj[:100]}...")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in chatml_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return len(chatml_data)


if __name__ == "__main__":
    input_file = "zig_dataset.jsonl"
    output_file = "zig_dataset_chatml.jsonl"
    
    count = convert_to_chatml(input_file, output_file)
    print(f"Convers√£o conclu√≠da! {count} conversas salvas em {output_file}")
```

### Resultado

Este script gera um arquivo chamado `zig_dataset_chatml.jsonl` compat√≠vel com:

* **Unsloth**
* **Axolotl**
* **SFT frameworks compat√≠veis com chat format**

Agora, o modelo sabe exatamente onde o usu√°rio fez uma pergunta e onde ele (como assistente) deve responder.

---

## Etapa 4 ‚Äî Treinando com Unsloth e Qwen 2.5

Para realizar o fine-tuning do nosso modelo com o dataset gerado, optamos por usar a biblioteca **[Unsloth](https://github.com/unslothai/unsloth)**. Ela √© uma otimiza√ß√£o sobre a TRL (Transformers Reinforcement Learning) que permite treinar modelos de forma mais leve, eficiente e r√°pida, especialmente com suporte a LoRA + 4bit quantization.

### Por que Unsloth?

* ‚úÖ Otimiza√ß√£o de mem√≥ria (QLoRA).
* ‚úÖ Treinamento r√°pido, mesmo em GPUs com 24GB de VRAM.
* ‚úÖ Suporte direto a modelos em 4bit (quantizados).
* ‚úÖ Integra√ß√£o com chat templates prontos, como os da fam√≠lia Qwen.

---

### Instala√ß√£o dos pacotes necess√°rios:

```bash
pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo
pip install sentencepiece protobuf datasets huggingface_hub hf_transfer wandb
pip install --no-deps unsloth
```

---

### Carregando o modelo

Neste projeto, usamos o modelo **Qwen2.5 3B Instruct**. Ele √© uma vers√£o compacta e instru√≠da da fam√≠lia Qwen, ideal para fine-tuning com LoRA.

```python
from unsloth import FastLanguageModel
import torch
max_seq_length = 4096 # Tamanho da janela de contexto, quanto maior o contexto maior consumo de vram e mais demorado
dtype = None # Podemos deixar None para otimiza√ß√£o automatica
load_in_4bit = True # Vamos carregar em 4bits para quantiza√ß√£o automatica, LORA √© 8bits e QLORA √© 4bits (menor consumo, menor precis√£o)

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "Qwen/Qwen2.5-3B-Instruct", # Aqui carregamos o modelo do hugginface
    max_seq_length = max_seq_length, 
    dtype = dtype, 
    load_in_4bit = load_in_4bit, 
)

# Agora vamos carregar o lora que explicando muito por cima √© criar uma copia menor do modelo que vai ser treinada.
# Para uma explica√ß√£o mais completa sobre lora indico esse artigo: N√£o, aumentar o Alpha significa multiplicar os pesos LoRA por um n√∫mero maior. Especificamente, Œ±/r. Para r=4, Œ±=8 o efeito ser√° 2x. O artigo tamb√©m menciona: 

model = FastLanguageModel.get_peft_model(
    model, # Passamos o modelo
    r = 128, # Aqui vamos definir o tamanho das matrizes, quanto maior mais memoria e mais conteudo √© armazenado pelo modelo
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj","lm_head", "embed_tokens"],
    # Aqui vamos definir quais camadas v√£o ser treinadas, adicionei "lm_head" e "embed_tokens", que aumentam o consumo de memoria e tempo de treinamento, mas melhoram o desempenho para aprender novos conteudos.
    lora_alpha = 128, # Aqui vamos definir a importancia das matrizes, quanto maior significa que maior peso vai ser dado a matriz de r
    lora_dropout = 0, # Supports any, but = 0 is optimized 
    bias = "none",    # Supports any, but = "none" is optimized
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

from unsloth.chat_templates import get_chat_template

# Unsloth j√° define o chat template de cada modelo ent√£o precisamos apenas setar a vers√£o 

tokenizer = get_chat_template(
    tokenizer,
    chat_template = "qwen2.5",
)

# Fun√ß√£o respons√°vel por converter de json para chatml aplicando o chat template do tokenizer.
def formatting_prompts_func(examples):
    convos = examples["conversations"]
    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]
    return { "text" : texts, }
pass

from datasets import load_dataset
# Vamos carregar o dataset zig_dataset_chatml.jsonl
dataset = load_dataset("json", data_files="zig_dataset_chatml.jsonl", split = "train")

# Vamos verificar as respostas do modelo antes do treinamento vou pegar algumas amostras do dataset.
import json
import random
from unsloth.chat_templates import get_chat_template

# Enable native 2x faster inference
FastLanguageModel.for_inference(model)

# Fun√ß√£o para processar exemplos e executar o modelo
def process_example(example):
    # Extrair a pergunta e resposta esperada do exemplo
    user_content = example["conversations"][0]["content"]
    expected_response = example["conversations"][1]["content"]
    
    # Criar a mensagem para o modelo
    messages = [
        {"role": "user", "content": user_content},
    ]
    
    # Preparar entrada para o modelo
    inputs = tokenizer.apply_chat_template(
        messages,
        tokenize = True,
        add_generation_prompt = True,  # Must add for generation
        return_tensors = "pt",
    ).to("cuda")
    
    # Gerar resposta com o modelo
    outputs = model.generate(
        input_ids = inputs, 
        max_new_tokens = 256, 
        use_cache = True,
        temperature = 0.7,  # Reduzir a temperatura para respostas mais determin√≠sticas
        min_p = 0.1
    )
    
    # Decodificar a resposta
    model_response = tokenizer.batch_decode(outputs)[0]
    
    # Exibir resultados
    print("\nPergunta:")
    print(user_content)
    print("\nResposta:")
    print(model_response)
    print("\nResposta esperada:")
    print(expected_response)
    print("\n" + "-"*80)

# Carregar o dataset
filename = "zig_dataset_chatml.jsonl"
dataset = []

try:
    with open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try:
                    example = json.loads(line)
                    dataset.append(example)
                except json.JSONDecodeError:
                    continue
    
    print(f"Carregados {len(dataset)} exemplos do dataset.\n")
    
    # Usar perguntas pr√©-definidas para avaliar o modelo (usando √≠ndices fixos)
    # Definir os √≠ndices das perguntas que queremos usar para avalia√ß√£o
    test_indices = [0, 1, 5, 10, 20]  # Voc√™ pode modificar estes √≠ndices conforme necess√°rio
    
    # Verificar se os √≠ndices est√£o dentro do intervalo v√°lido
    valid_indices = [idx for idx in test_indices if idx < len(dataset)]
    if len(valid_indices) < len(test_indices):
        print(f"Aviso: Alguns √≠ndices estavam fora do intervalo. Usando {len(valid_indices)} exemplos.")
    
    # Obter os exemplos pelos √≠ndices
    test_examples = [dataset[idx] for idx in valid_indices]
    
    # Processar cada exemplo
    for i, example in enumerate(test_examples, 1):
        print(f"Exemplo {i}/{len(test_examples)}")
        process_example(example)
        
except Exception as e:
    print(f"Erro ao processar o dataset: {e}")


from unsloth.chat_templates import standardize_sharegpt
# Existem dois formatos que s√£o bem parecidos sharegpt e o padr√£o da openai a diferen√ßa entre eles √© somente o nome das chaves ent√£o unsloth criou standardize_sharegpt que suporta as duas vers√µes.
dataset = standardize_sharegpt(dataset)
dataset = dataset.map(formatting_prompts_func, batched = True,)

print("Dataset Original:")
print(dataset[5]["conversations"])

print("Ap√≥s passar pelo chat template:")
print(dataset[8]["text"])

from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
from unsloth import UnslothTrainer, UnslothTrainingArguments

trainer = UnslothTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,

    args = UnslothTrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 8,

        warmup_steps = 10,
        num_train_epochs = 3,

        embedding_learning_rate = 1e-5, # Aqui definimos o treinamento do embed layer menor do que a taxa de treinamento do restante do modelo.
        learning_rate = 5e-5,

        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "wandb", # Wandb para Logs
    ),
)

trainer_stats = trainer.train()
```
---

## Etapa 5 ‚Äî Avalia√ß√£o: Antes e Depois do Fine-Tuning

Para validar se o modelo aprendeu com o nosso dataset de Zig 0.14, realizamos um teste simples:

* **Selecionamos 5 exemplos reais do dataset.**
* **Pedimos ao modelo que gerasse a resposta antes e depois do treinamento.**
* **Comparamos a sa√≠da gerada com a resposta esperada.**

---

### üß™ Exemplo 1

**Pergunta:**

> What is the purpose of doc comments in Zig?

**Antes do Fine-Tuning:**

> In Zig programming language, doc comments (also known as "documentation comments"...)
> (*Resposta longa, gen√©rica, e baseada em estilos de outras linguagens*)

**Depois do Fine-Tuning:**

> Doc comments provide documentation for code, allowing other developers to understand what the code does without having to read through the implementation...<|im\_end|>

**Esperado:**

> Doc comments in Zig are used to provide documentation for the code...

‚úÖ *Melhoria: Resposta mais direta, mais pr√≥xima do estilo esperado no dataset.*

---

### üß™ Exemplo 2

**Pergunta:**

> Write a Zig code snippet that demonstrates the use of primitive type literals and assignment.

**Antes:**

```zig
const intLiteral: i32 = 42;
const floatLiteral: f64 = 3.14;
// ...
```

**Depois:**

```zig
const x: i32 = 10; const y: u32 = 20; const z: f64 = 30.5;<|im_end|>
```

**Esperado:**

```zig
const a: i32 = 10; const b: u32 = 20; const c: f64 = 30.5;
```

‚úÖ *Melhoria: Resposta curta, direta e aderente ao estilo do dataset.*

---

### üß™ Exemplo 3

**Pergunta:**

> What is the purpose of the `defer` statement in Zig?

**Antes:**

> Longa explica√ß√£o com m√∫ltiplos exemplos e explica√ß√µes repetitivas.

**Depois:**

> The `defer` statement is used to run code at function exit, regardless of how the function exits. It is typically used for cleanup tasks...<|im\_end|>

**Esperado:**

> In Zig, the `defer` statement is used to defer the execution of a block of code until the end of the current scope...

‚úÖ *Melhoria clara na concis√£o e alinhamento conceitual.*

---

### üß™ Exemplo 4

**Pergunta:**

> What is the primary purpose of the Zig Standard Library?

**Antes:**

> Lista longa com 6 t√≥picos explicativos, linguagem gen√©rica.

**Depois:**

> The primary purpose of the Zig Standard Library is to provide a set of useful functions and types...<|im\_end|>

**Esperado:**

> ...commonly used algorithms, data structures, and definitions...

‚úÖ *Mais objetivo, alinhado com a resposta-alvo.*

---

### üß™ Exemplo 5

**Pergunta:**

> In which places are doc comments allowed in Zig?

**Antes:**

> Explica√ß√£o confusa, menciona aus√™ncia de suporte, fala de doxygen e IDEs.

**Depois:**

> Doc comments can be used at the start of a file, at the start of a function...<|im\_end|>

**Esperado:**

> Doc comments are only allowed in certain places, such as before a function...

‚úÖ *Maior precis√£o ap√≥s o treinamento.*

---
## Etapa 6 ‚Äî Expans√£o do Dataset e Novo Ciclo de Treinamento

Ap√≥s a primeira rodada de testes, percebi que embora as respostas do modelo estivessem **melhores, mais concisas e com estilo alinhado**, elas ainda **careciam de profundidade e precis√£o conceitual** em alguns casos.

Um dos motivos era claro: **nosso dataset original tinha apenas 750 exemplos**, o que √© muito pouco para ensinar uma linguagem inteira como Zig, mesmo com LoRA.

### üîç Estrat√©gia: Aumentar a Diversidade e Cobertura dos Dados

Para resolver isso, decidi **aumentar significativamente o volume e a variedade de fontes**, utilizando n√£o apenas a documenta√ß√£o oficial, mas tamb√©m materiais educacionais da comunidade Zig.

### üìö Novas Fontes Inclu√≠das

| Fonte                                                                                     | Descri√ß√£o                                        | Linhas |
| ----------------------------------------------------------------------------------------- | ------------------------------------------------ | ------ |
| [zigcc/zig-cookbook](https://github.com/zigcc/zig-cookbook)                               | Receitas pr√°ticas em Zig                         | 1.596  |
| [jkitajima/learning-zig-karlseguin](https://github.com/jkitajima/learning-zig-karlseguin) | Curso completo baseado nos textos de Karl Seguin | 1.040  |
| [sobeston/zig.guide](https://github.com/sobeston/zig.guide)                               | Guia introdut√≥rio em Markdown                    | 456    |
| [ziglang.org/documentation/master](https://ziglang.org/documentation/master/)             | Documenta√ß√£o oficial mais atual                  | 1.875  |
| [ziglang.org/release-notes 0.14](https://ziglang.org/download/0.14.0/release-notes.html)  | Notas de vers√£o da release-alvo                  | 846    |

üìà **Total: 5.817 exemplos + training-prompts.jsonl original**

---

### ‚öôÔ∏è Mesma Pipeline, Agora com Mais Dados

O processo de gera√ß√£o de exemplos segue o mesmo fluxo:

1. **Busca por arquivos `.md` e `.mdx`** nos reposit√≥rios.
2. **Divis√£o por se√ß√µes e t√≥picos.**
3. **Gera√ß√£o de perguntas e respostas com LLM (Novita).**
4. **Valida√ß√£o parcial por amostragem.**
5. **Convers√£o para o formato ChatML (`chat_template`) com Unsloth.**

Al√©m disso, reaproveitamos o dataset `training-prompts.jsonl` usado por Akita como parte do oversampling para manter coer√™ncia com os exemplos iniciais.

---

### ‚ñ∂Ô∏è Treinamento com o Novo Dataset

Com os novos dados integrados, o **script de treinamento permanece exatamente o mesmo**, apenas com o caminho do novo JSONL atualizado:

```python
dataset = load_dataset("json", data_files="zig_dataset_chatml.jsonl", split = "train")
```

Todo o restante do pipeline (aplica√ß√£o do chat template, formata√ß√£o, configura√ß√£o do LoRA e `UnslothTrainer`) continua funcional, escalando bem mesmo com um dataset significativamente maior.

Excelente avan√ßo! Com essa etapa, voc√™ demonstra de forma clara o **ponto de partida antes do fine-tuning** com o novo dataset do Hugging Face. Isso fortalece seu artigo como um caso real de melhoria de LLM por LoRA.

Aqui est√° a continua√ß√£o do artigo, explicando essa etapa de forma did√°tica e estruturada:

---

## Etapa 7 ‚Äî Avalia√ß√£o Inicial com Dataset do Hugging Face

Antes de realizar o novo fine-tuning, decidimos medir o desempenho atual do modelo **Qwen 2.5** com o dataset j√° formatado e publicado no Hugging Face:

üìÅ [JJhooww/ziglang\_sharegpt](https://huggingface.co/datasets/JJhooww/ziglang_sharegpt)

Esse dataset cont√©m dois splits:

* **Treino** (`train`): usado para fine-tuning.
* **Avalia√ß√£o** (`test`): usado para medir o desempenho antes e depois do treinamento.

---

### ‚öôÔ∏è Procedimento

* Carregamos o dataset usando `datasets.load_dataset`.
* Avaliamos os **primeiros 5 exemplos do split de teste**, pedindo que o modelo gere uma resposta para cada pergunta.
* Comparamos a **resposta gerada** com a **resposta esperada**.

---

### üìä Resultados Antes do Treinamento

#### üîπ Exemplo 1

**Pergunta:**

> What is your knowledge cutoff for Zig?

**Resposta do modelo:**
Fala sobre Zigbee e diz que n√£o tem dados sobre Zig ap√≥s 2021.

**Resposta esperada:**

> I know about Zig up to version 0.14.0 (2025), with access to the latest documentation.

‚ùå **Resultado:** resposta incorreta e fora do tema.

---

#### üîπ Exemplo 2

**Pergunta:**

> What is the latest version of the Zig programming language?

**Resposta do modelo:**

> Zig 1.7.0 (2023)

**Resposta esperada:**

> Zig 0.14.0, publicado em 2025

‚ùå **Resultado:** desatualizado e incorreto.

---

#### üîπ Exemplo 3

**Pergunta:**

> Can you show me how to use feature X from Zig 0.14.0?

**Resposta do modelo:**
Resposta vaga, gen√©rica, com exemplos inventados de "zigconfig.zig".

**Resposta esperada:**

> Explica√ß√£o direta e precisa da feature X conforme a vers√£o 0.14.0.

‚ùå **Resultado:** modelo responde com "placeholders", sem conhecimento real da vers√£o.

---

#### üîπ Exemplo 4

**Pergunta:**

> Which version of the Zig programming language are you familiar with?

**Resposta do modelo:**

> Zig 1.7.0, √∫ltima vers√£o conhecida em 2023

**Resposta esperada:**

> Zig 0.14.0, com base nos documentos de 2025

‚ùå **Resultado:** novamente desatualizado.

---

#### üîπ Exemplo 5

**Pergunta:**

> What is your Zig knowledge cutoff?

**Resposta do modelo:**

> Treinamento at√© 2021

**Resposta esperada:**

> At√© 0.14.0 (2025), com acesso √† documenta√ß√£o mais recente

‚ùå **Resultado:** conhecimento incorreto e desatualizado.

---

## Etapa 8 ‚Äî Tempo de Treinamento e Custo: Efici√™ncia Real com GPU Gratuita

Com o novo dataset e valida√ß√£o por √©poca ativada, o tempo de treinamento naturalmente aumentou. Antes, com \~750 exemplos e sem `eval_dataset`, o tempo total para 3 √©pocas era de aproximadamente **15 minutos**.

Ap√≥s a expans√£o para mais de **5.800 pares de dados**, divididos entre treino e teste, o tempo subiu para:

‚è±Ô∏è **1 hora, 10 minutos e 36 segundos**
üìà **3 √©pocas completas**
üîÅ Avalia√ß√£o e salvamento do modelo ao final de cada √©poca.

---

### üöÄ Hardware e Infraestrutura Utilizada

Apesar do aumento, o custo de toda a opera√ß√£o se manteve **zero**, gra√ßas a uma combina√ß√£o inteligente de ferramentas e infraestrutura gratuita:

| Recurso                            | Finalidade                                             | Custo |
| ---------------------------------- | ------------------------------------------------------ | ----- |
| üß† **Novita.ai**                   | Gera√ß√£o de dataset com LLM (15 USD de cr√©dito inicial) | R\$ 0 |
| üñ•Ô∏è **Google Colab** (GPU T4 16GB) | Treinamento com Unsloth e LoRA                         | R\$ 0 |
| üíæ **Hugging Face Hub**            | Armazenamento e versionamento do dataset               | R\$ 0 |
| üß™ **Weights & Biases**            | Logs e m√©tricas de treino                              | R\$ 0 |

üí° *Ou seja, at√© o momento, n√£o gastamos um √∫nico centavo


